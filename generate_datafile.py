# è¯»å–excelæ–‡ä»¶ï¼Œåˆ’åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæ¯ä¸€æ¡ä¿¡æ¯ä¸­åŒ…å«äº†æ•°æ®ä¸¤ä¸ªæœŸç›¸çš„åœ°å€ï¼ˆdata&maskï¼‰
# å¯ä»¥æŒ‡labelåˆ—ï¼Œæ ¹æ®ä¸åŒçš„labelè¿›è¡Œåˆ’åˆ†ï¼Œé»˜è®¤æ˜¯TRGï¼Œä¹Ÿå°±æ˜¯column=1
import os
from os.path import join
import re
import random
import pandas as pd
import json
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.model_selection import StratifiedKFold
random.seed(42)

# .niiæ‰€åœ¨çš„å¤§æ–‡ä»¶å¤¹
BASE_DIR="/home/laicy/data/exterenal_validation_set/CT_data/nii_resample/"  #ä¿®æ”¹
OUTPUT_DIR = "/home/laicy/data/train_set/CT_data/data_splits/response"
TITAN_DIR="/home/laicy/data/exterenal_validation_set/titan_features/"
os.makedirs(OUTPUT_DIR, exist_ok=True)
def parse_excel_info(filepath):
    """è§£æExcelæ–‡ä»¶åè·å–æ²»ç–—æ–¹å¼å’Œæ¨¡æ€ç±»å‹"""
    filename = os.path.basename(filepath)
    parts = filename.split()
    return {
        'treatment': parts[0],  # æ–°è¾…åŠ©å…ç–«/æ–°è¾…åŠ©åŒ–ç–—
        'modality': parts[1],   # æ¨¡æ€ç¼ºå¤±/æ¨¡æ€é½å…¨
        'excel_path': filename
    }

def get_ct_names(ct_path):
    #è·å–ctçš„åå­—ï¼Œè¿”å›çš„æ˜¯CTåçš„åˆ—è¡¨
    #ct_pathï¼šctåå­—çš„åˆ—è¡¨
    #ct_names:è¿”å›çš„æ˜¯ctåå­—çš„åˆ—è¡¨

    nums=os.listdir(ct_path)
    ct_names=[]
    for num in nums:
        match = re.match(r"(\d+-\d+-Pre)",num)
        if match:
            ct_names.append(match.group(1))
    return list(set(ct_names))

def load_and_clean_excels(excel_files):
    """åŠ è½½å¹¶æ¸…æ´—Excelæ•°æ®"""
    dfs = []
    
    for filepath in excel_files:
        info = parse_excel_info(filepath)
        try:
            df = pd.read_excel(filepath,dtype={0:str,1:str})
            
            # ç”Ÿæˆæ ·æœ¬åç§°ï¼Œå¹¶ç¡®ä¿å…¶ä¸ºå­—ç¬¦ä¸²ç±»å‹
            sample_name_1 = df.iloc[:, 0].astype(str).str.strip()
            sample_name_2 = df.iloc[:, 1].astype(str).str.strip()
            df['sample_name'] = sample_name_1.values + "-" + sample_name_2.values 
            
            # æ·»åŠ å…ƒæ•°æ®
            df['treatment'] = info['treatment']
            df['modality'] = info['modality']
            
            # å¤„ç†æŒ‡æ ‡åˆ—ï¼ˆä»ç¬¬4åˆ—å¼€å§‹ï¼‰
            metric_cols = df.columns[3:]
            for col in metric_cols:
                # ç»Ÿä¸€ç©ºå€¼è¡¨ç¤º
                df[col] = df[col].replace(['', 'NA', 'NaN', 'None'], np.nan)
                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                try:
                    df[col] = pd.to_numeric(df[col])
                except:
                    pass
            
            # ä¿ç•™éœ€è¦çš„åˆ—
            keep_cols = metric_cols.tolist()
            dfs.append(df[keep_cols])
            
        except Exception as e:
            print(f"é”™è¯¯: åŠ è½½ {filepath} å¤±è´¥ - {str(e)}")
            continue
    
    if not dfs:
        raise ValueError("æ‰€æœ‰Excelæ–‡ä»¶åŠ è½½å¤±è´¥")
    
    return pd.concat(dfs, ignore_index=True)

def build_file_paths(base_name, modality, treatment):
    """æ„å»ºæ–‡ä»¶è·¯å¾„å­—å…¸"""
    base_dir = os.path.join(BASE_DIR, modality, treatment)
    # åŠ¨è„‰æœŸæ–‡ä»¶
    artery_file = f"{base_name}-Pre-A.nii.gz"
    artery_mask = f"{base_name}-Pre-AM.nii.gz"

    # é™è„‰æœŸæ–‡ä»¶
    vein_file = f"{base_name}-Pre-V.nii.gz"
    vein_mask = f"{base_name}-Pre-VM.nii.gz"
    titan_path=os.path.join(TITAN_DIR, f"{base_name}.npy")
    if os.path.exists(titan_path):
        titan_path=titan_path
    else:
        titan_path=None                    
    return {
        'artery_path': os.path.join(base_dir, "åŠ¨è„‰æœŸ", artery_file),
        'artery_mask': os.path.join(base_dir, "åŠ¨è„‰æœŸ", artery_mask),
        'vein_path': os.path.join(base_dir, "é™è„‰æœŸ", vein_file),
        'vein_mask': os.path.join(base_dir, "é™è„‰æœŸ", vein_mask),
        "titan_path":titan_path
    }

def verify_files(file_dict):
    """éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    return all(os.path.exists(path) for path in file_dict.values())

def create_dataset(df):
    """åˆ›å»ºæœ€ç»ˆæ•°æ®é›†,å¹¶å¤„ç†æ ·æœ¬ç¼ºå¤±çš„æƒ…å†µ"""
    records = []
    missing_files = []
    
    for _, row in df.iterrows():
        try:
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            paths = build_file_paths(row['sample_name'], row['modality'], row['treatment'])
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            missing = [k for k, v in list(paths.items())[:-1] if not os.path.exists(v)]
            if missing:
                missing_files.append({
                    'sample': row['sample_name'],
                    'missing_files': missing,
                    'treatment': row['treatment'],
                    'modality': row['modality']
                })
                continue
            
            # æ„å»ºè®°å½•
            record = {
                'sample_name': row['sample_name'],
                'treatment': row['treatment'],
                'modality': row['modality'],
                **paths
            }
            
            # æ·»åŠ æŒ‡æ ‡æ•°æ®ï¼ˆè‡ªåŠ¨å¤„ç†NaNï¼‰
            for col in df.columns:
                if col not in ['sample_name', 'treatment', 'modality']:
                    record[col] = row[col] if pd.notna(row[col]) else None
            
            records.append(record)
            
        except Exception as e:
            print(f"å¤„ç†æ ·æœ¬ {row.get('sample_name', 'æœªçŸ¥')} å‡ºé”™: {str(e)}")
            continue
    
    # ä¿å­˜ç¼ºå¤±æ–‡ä»¶ä¿¡æ¯
    if missing_files:
        missing_report = os.path.join(OUTPUT_DIR, "missing_files_report.json")
        with open(missing_report, 'w') as f:
            json.dump(missing_files[:100], f, indent=4,ensure_ascii=False)  # æœ€å¤šä¿å­˜100æ¡è®°å½•
        print(f"\nè­¦å‘Š: å…± {len(missing_files)} ä¸ªæ ·æœ¬ç¼ºå¤±æ–‡ä»¶ï¼Œè¯¦è§ {missing_report}")
    
    return pd.DataFrame(records), missing_files

def clean_dataframe(df, metric_col, drop_na=True):
    """
    æ¸…æ´—DataFrameï¼Œå¤„ç†ç¼ºå¤±å€¼å¹¶æ£€æŸ¥åˆ—ç´¢å¼•åˆæ³•æ€§

    å‚æ•°:
    - df: pandas DataFrame
    - metric_col: å•ä¸ªåˆ—å/ç´¢å¼•ï¼Œæˆ–åˆ—å/ç´¢å¼•çš„åˆ—è¡¨
    - drop_na: æ˜¯å¦åˆ é™¤ç¼ºå¤±å€¼è¡Œï¼ˆé»˜è®¤Trueï¼‰

    è¿”å›:
    - æ¸…æ´—åçš„DataFrame
    """
    import pandas as pd

    # æ ‡å‡†åŒ– metric_col ä¸ºåˆ—è¡¨
    if isinstance(metric_col, (int, str)):
        metric_cols = [metric_col]
    elif isinstance(metric_col, list):
        metric_cols = metric_col
    else:
        raise TypeError("metric_col åº”ä¸º intã€str æˆ–å…¶åˆ—è¡¨")

    # æ£€æŸ¥åˆ—ç´¢å¼•åˆæ³•æ€§
    valid_cols = df.columns
    for col in metric_cols:
        if isinstance(col, int):
            if col >= len(valid_cols):
                raise ValueError(f"metric_col ç´¢å¼• {col} è¶…å‡ºèŒƒå›´ (æœ€å¤§ç´¢å¼•: {len(valid_cols)-1})")
        elif isinstance(col, str):
            if col not in valid_cols:
                raise ValueError(f"metric_col åç§° '{col}' ä¸å­˜åœ¨äº DataFrame åˆ—ä¸­")
        else:
            raise TypeError(f"metric_col ä¸­çš„å…ƒç´ å¿…é¡»ä¸º int æˆ– strï¼Œä½†æ”¶åˆ°: {type(col)}")

    # è·å–åˆ—åï¼ˆæ— è®º metric_col æ˜¯ç´¢å¼•è¿˜æ˜¯åˆ—åï¼‰
    metric_col_names = [
        df.columns[col] if isinstance(col, int) else col
        for col in metric_cols
    ]

    if drop_na:
        df_clean = df.dropna(subset=metric_col_names)
        dropped = len(df) - len(df_clean)
        if dropped > 0:
            print(f"âš ï¸ åˆ é™¤äº† {dropped} è¡ŒåŒ…å«NaNå€¼çš„æ•°æ®")
    else:
        if df[metric_col_names].isna().any().any():
            raise ValueError("æŒ‡å®šçš„æŒ‡æ ‡åˆ—ä¸­åŒ…å« NaNï¼Œè¯·è®¾ç½® drop_na=True æˆ–æ‰‹åŠ¨å¤„ç†")
        df_clean = df.copy()

    return df_clean

def stratified_split_by_metrics(df, metric_col=[8,-1], test_size=0.2, drop_na=True):
    """
    åŸºäºæŒ‡å®šæŒ‡æ ‡åˆ—åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶ç»Ÿè®¡ä¸åŒç±»æŒ‡æ ‡çš„ä¸ªæ•°
    """
    RANDOM_SEED = 42
    print("ğŸ“‹ åŸå§‹æ•°æ®é¢„è§ˆ:")
    print(df.head())

    df_clean = clean_dataframe(df, metric_col, drop_na)
    strata = df_clean.iloc[:, metric_col].astype(str)

    print("\nğŸ“Š åŸå§‹æ•°æ®ä¸­å„ç±»æŒ‡æ ‡çš„ä¸ªæ•°:")
    print(strata.value_counts())

    train_df, test_df = train_test_split(
        df_clean,
        test_size=test_size,
        random_state=RANDOM_SEED,
        stratify=strata
    )

    print("\nâœ… è®­ç»ƒé›†ä¸­å„ç±»æŒ‡æ ‡çš„ä¸ªæ•°:")
    print(train_df.iloc[:, metric_col].astype(str).value_counts())
    print("\nâœ… æµ‹è¯•é›†ä¸­å„ç±»æŒ‡æ ‡çš„ä¸ªæ•°:")
    print(test_df.iloc[:, metric_col].astype(str).value_counts())

    return train_df, test_df
def kfold_split(df, n_splits=5, stat_col=8, drop_na=True):
    """
    ç¬¬7åˆ—æ˜¯titan_pathï¼ˆæ¨¡æ€åœ°å€ï¼‰ï¼Œç¬¬8åˆ—æ˜¯TRGæ ‡ç­¾ã€‚
    åˆ’åˆ†æ¯æŠ˜åŒ…å«æ¨¡æ€é½å…¨å­é›†å’Œå¹³è¡¡çš„æ•´ä½“æ•°æ®é›†ã€‚
    
    å‚æ•°:
    - df: è¾“å…¥DataFrame
    - n_splits: æŠ˜æ•°
    - stat_col: ç”¨äºåˆ†å±‚çš„åˆ—ç´¢å¼•æˆ–åˆ—å
    - drop_na: æ˜¯å¦åˆ é™¤åŒ…å«NaNçš„è¡Œ
    
    è¿”å›:
    - åŒ…å«å„æŠ˜æ•°æ®çš„åˆ—è¡¨ï¼Œæ¯æŠ˜åŒ…å«train_full, val_full, train_all, val_all
    """
    from sklearn.model_selection import StratifiedKFold
    import pandas as pd
    
    df_clean = clean_dataframe(df, stat_col, drop_na)
    stat_col_name = df_clean.columns[stat_col] if isinstance(stat_col, int) else stat_col
    titan_col_name = df_clean.columns[7] if isinstance(7, int) else 7  # ç¬¬7åˆ—æ˜¯titan_path

    # æ‹†åˆ†é½å…¨/ç¼ºå¤±æ¨¡æ€æ•°æ®
    full_modal_df = df_clean[df_clean[titan_col_name].notna()].copy()

    missing_modal_df = df_clean.copy()

    # æ£€æŸ¥æ¨¡æ€é½å…¨æ•°æ®çš„ç±»åˆ«åˆ†å¸ƒ
    strata = full_modal_df[stat_col_name]
    min_class_size = strata.value_counts().min()
    if min_class_size < n_splits:
        raise ValueError(f"âŒ æ¨¡æ€é½å…¨æ•°æ®ä¸­ï¼Œæœ€å°ç±»åˆ«æ ·æœ¬æ•°ä¸º {min_class_size}ï¼Œå°äº n_splits={n_splits}ï¼Œè¯·é™ä½æŠ˜æ•°æˆ–æ£€æŸ¥æ•°æ®ã€‚")

    # å…ˆå¯¹æ¨¡æ€é½å…¨æ•°æ®è¿›è¡Œåˆ†å±‚åˆ’åˆ†
    full_kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    full_folds = []
    for _, val_idx in full_kf.split(full_modal_df, strata):
        full_folds.append(val_idx)

    # ç„¶åå¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œåˆ†å±‚åˆ’åˆ†
    all_kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    all_folds = []
    for _, val_idx in all_kf.split(missing_modal_df, missing_modal_df[stat_col_name]):
        all_folds.append(val_idx)

    folds = []
    print("\nğŸ“ å¼€å§‹ K æŠ˜åˆ’åˆ†:")
    
    for i in range(n_splits):
        # æ¨¡æ€é½å…¨æ•°æ®çš„åˆ’åˆ†
        val_full_idx = full_folds[i]
        train_full_idx = [idx for fold in full_folds[:i] + full_folds[i+1:] for idx in fold]
        
        val_full = full_modal_df.iloc[val_full_idx].reset_index(drop=True)
        train_full = full_modal_df.iloc[train_full_idx].reset_index(drop=True)
        
        # æ•´ä¸ªæ•°æ®é›†ï¼ˆå«ç¼ºå¤±æ¨¡æ€ï¼‰çš„åˆ’åˆ†
        val_all_idx = all_folds[i]
        train_all_idx = [idx for fold in all_folds[:i] + all_folds[i+1:] for idx in fold]
        
        val_all = missing_modal_df.iloc[val_all_idx].reset_index(drop=True)
        train_all = missing_modal_df.iloc[train_all_idx].reset_index(drop=True)
        
        # ç¡®ä¿é½å…¨æ•°æ®æ˜¯å®Œæ•´æ•°æ®çš„å­é›†
        # é€šè¿‡ç´¢å¼•æ˜ å°„æ¥ä¿è¯è¿™ä¸ªå…³ç³»
        full_in_all_train = train_full.index.to_series().map(lambda x: x in train_all.index).all()
        full_in_all_val = val_full.index.to_series().map(lambda x: x in val_all.index).all()
        
        if not (full_in_all_train and full_in_all_val):
            # å¦‚æœä¸æ»¡è¶³å­é›†å…³ç³»ï¼Œè°ƒæ•´å®Œæ•´æ•°æ®é›†åˆ’åˆ†
            train_all = pd.concat([train_all, train_full]).drop_duplicates().reset_index(drop=True)
            val_all = pd.concat([val_all, val_full]).drop_duplicates().reset_index(drop=True)
            # é‡æ–°è¿›è¡Œåˆ†å±‚åˆ’åˆ†ä»¥ä¿æŒç±»åˆ«å¹³è¡¡
            train_all_idx, val_all_idx = next(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42+i).split(missing_modal_df, missing_modal_df[stat_col_name]))
            train_all = missing_modal_df.iloc[train_all_idx].reset_index(drop=True)
            val_all = missing_modal_df.iloc[val_all_idx].reset_index(drop=True)
            # å†æ¬¡ç¡®ä¿å­é›†å…³ç³»
            train_all = pd.concat([train_all, train_full]).drop_duplicates().reset_index(drop=True)
            val_all = pd.concat([val_all, val_full]).drop_duplicates().reset_index(drop=True)

        print(f"\nğŸ“‚ Fold {i+1}:")
        print(f"æ¨¡æ€é½å…¨è®­ç»ƒé›†å¤§å°: {len(train_full)}, ç±»åˆ«åˆ†å¸ƒ:")
        print(train_full[stat_col_name].value_counts().sort_index())
        print(f"æ¨¡æ€é½å…¨éªŒè¯é›†å¤§å°: {len(val_full)}, ç±»åˆ«åˆ†å¸ƒ:")
        print(val_full[stat_col_name].value_counts().sort_index())
        
        print(f"æ¨¡æ€å«ç¼ºå¤±è®­ç»ƒé›†å¤§å°: {len(train_all)}, ç±»åˆ«åˆ†å¸ƒ:")
        print(train_all[stat_col_name].value_counts().sort_index())
        print(f"æ¨¡æ€å«ç¼ºå¤±éªŒè¯é›†å¤§å°: {len(val_all)}, ç±»åˆ«åˆ†å¸ƒ:")
        print(val_all[stat_col_name].value_counts().sort_index())

        folds.append({
            "fold_id": i + 1,
            "train_full": train_full,
            "val_full": val_full,
            "train_all": train_all,
            "val_all": val_all
        })

    return folds

def get_complete_data(df, stat_col=8, drop_na=True):
    """
    è¿”å›ç»™å®šæŒ‡æ ‡éƒ½å­˜åœ¨çš„æ‰€æœ‰æ•°æ®ï¼Œä»¥åŠtitan_pathå­˜åœ¨çš„æ‰€æœ‰æ•°æ®
    
    å‚æ•°:
    - df: è¾“å…¥DataFrame
    - stat_col: ç”¨äºæ£€æŸ¥çš„åˆ—ç´¢å¼•æˆ–åˆ—å
    - drop_na: æ˜¯å¦åˆ é™¤åŒ…å«NaNçš„è¡Œ
    
    è¿”å›:
    - åŒ…å«å®Œæ•´æ•°æ®å’Œæ¨¡æ€é½å…¨æ•°æ®çš„å­—å…¸
    """
    import pandas as pd
    
    df_clean = clean_dataframe(df, stat_col, drop_na)

    stat_col_name = df_clean.columns[stat_col] if isinstance(stat_col, int) else stat_col
    titan_col_name = df_clean.columns[7] if isinstance(7, int) else 7  # ç¬¬7åˆ—æ˜¯titan_path

    # è·å–æ‰€æœ‰ç»™å®šæŒ‡æ ‡éƒ½å­˜åœ¨çš„æ•°æ®ï¼ˆå®Œæ•´æ•°æ®ï¼‰
    complete_df = df_clean.copy()
    
    # è·å–titan_pathå­˜åœ¨çš„æ•°æ®ï¼ˆæ¨¡æ€é½å…¨æ•°æ®ï¼‰
    full_modal_df = df_clean[df_clean[titan_col_name].notna()].copy()

    print("\nğŸ“Š æ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
    print(f"å®Œæ•´æ•°æ®é›†å¤§å°: {len(complete_df)}, ç±»åˆ«åˆ†å¸ƒ:")
    print(complete_df[stat_col_name].value_counts().sort_index())
    print(f"æ¨¡æ€é½å…¨æ•°æ®é›†å¤§å°: {len(full_modal_df)}, ç±»åˆ«åˆ†å¸ƒ:")
    print(full_modal_df[stat_col_name].value_counts().sort_index())

    return {
        "complete_data": complete_df,
        "full_modal_data": full_modal_df
    }

if __name__ == '__main__':

    excel_files={
        "/home/laicy/data/exterenal_validation_set/CT_data/æ–°è¾…åŠ©åŒ–ç–— æ¨¡æ€é½å…¨ é¢„æµ‹ç»ˆç‚¹.xlsx",
        "/home/laicy/data/exterenal_validation_set/CT_data/æ–°è¾…åŠ©åŒ–ç–— æ¨¡æ€ç¼ºå¤± é¢„æµ‹ç»ˆç‚¹.xlsx",
        "/home/laicy/data/exterenal_validation_set/CT_data/æ–°è¾…åŠ©å…ç–« æ¨¡æ€é½å…¨ é¢„æµ‹ç»ˆç‚¹.xlsx",
        "/home/laicy/data/exterenal_validation_set/CT_data/æ–°è¾…åŠ©å…ç–« æ¨¡æ€ç¼ºå¤± é¢„æµ‹ç»ˆç‚¹.xlsx",
    }
    combined_df=load_and_clean_excels(excel_files)
    print(f"from {len(excel_files)} excel files, {len(combined_df)} records loaded.")
    full_dataset,missing=create_dataset(combined_df)
    if len(full_dataset)==0:
        raise ValueError("No data found after combining all excel files.")

    # folds = kfold_split(full_dataset,stat_col=-1)
    
    
    # # ä¿å­˜äº¤å‰éªŒè¯çš„æ‹†åˆ†ç»“æœ
    # for i, fold in enumerate(folds):
    #     fold["train_full"].to_json(join(OUTPUT_DIR,f"fold_{i+1}_train_full.json"), orient='records', indent=4,force_ascii=False)
    #     fold["val_full"].to_json(join(OUTPUT_DIR,f"fold_{i+1}_val_full.json"), orient='records', indent=4,force_ascii=False)
    #     fold["train_all"].to_json(join(OUTPUT_DIR,f"fold_{i+1}_train_all.json"), orient='records', indent=4,force_ascii=False)
    #     fold["val_all"].to_json(join(OUTPUT_DIR,f"fold_{i+1}_val_all.json"), orient='records', indent=4,force_ascii=False)
    #     fold_num = 1

    # å¤–éƒ¨éªŒè¯é›†æ—¶ å–æ¶ˆæ³¨é‡Š
    data=get_complete_data(full_dataset)
    full_modal_data=data["full_modal_data"]
    complete_data=data["complete_data"]        
    # å¤–éƒ¨éªŒè¯é›†ä¿å­˜JSONæ ¼å¼
    complete_data.to_json(os.path.join(OUTPUT_DIR, "all_dataset_ex.json"), orient='records', indent=4,force_ascii=False)  #æ‰€æœ‰æ•°æ®é›†
    full_modal_data.to_json(os.path.join(OUTPUT_DIR, "full_dataset_ex.json"), orient='records', indent=4,force_ascii=False) #æ¨¡æ€é½å…¨çš„æ•°æ®